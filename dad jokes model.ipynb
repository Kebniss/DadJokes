{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchnlp.samplers import BucketBatchSampler\n",
    "from torchnlp.datasets import snli_dataset\n",
    "from torchnlp.utils import datasets_iterator\n",
    "from torchnlp.text_encoders import IdentityEncoder, CharacterEncoder, StaticTokenizerEncoder\n",
    "from torchnlp import word_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm as tq\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "\n",
    "def in_ipynb():\n",
    "    try:\n",
    "        cls = get_ipython().__class__.__name__\n",
    "        return cls == 'ZMQInteractiveShell'\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def in_notebook():\n",
    "    try:\n",
    "        from ipykernel.kernelapp import IPKernelApp\n",
    "        return IPKernelApp.initialized()\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def clear_tqdm():\n",
    "    inst = getattr(tq.tqdm, '_instances', None)\n",
    "    if not inst: return\n",
    "    try:\n",
    "        for i in range(len(inst)): inst.pop().close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if in_notebook():\n",
    "    def tqdm(*args, **kwargs):\n",
    "        clear_tqdm()\n",
    "        return tq.tqdm(*args, file=sys.stdout, **kwargs)\n",
    "    def trange(*args, **kwargs):\n",
    "        clear_tqdm()\n",
    "        return tq.trange(*args, file=sys.stdout, **kwargs)\n",
    "else:\n",
    "    from tqdm import tqdm, trange\n",
    "    tnrange=trange\n",
    "    tqdm_notebook=tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterByteEncoder(StaticTokenizerEncoder):\n",
    "    \"\"\" Encodes text into a tensor by encoding into bytes and splitting the text into individual \n",
    "    characters.\n",
    "\n",
    "    Args:\n",
    "        sample (list of strings): Sample of data to build dictionary on\n",
    "        min_occurrences (int, optional): Minimum number of occurrences for a token to be added to\n",
    "          dictionary.\n",
    "        append_eos (bool, optional): If `True` append EOS token onto the end to the encoded vector.\n",
    "    \"\"\"\n",
    "    # TODO add start and end token\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'tokenize' in kwargs:\n",
    "            raise TypeError('CharacterEncoder defines a tokenize callable per character')\n",
    "        super().__init__(*args, tokenize=self._tokenize, **kwargs)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        tokens = [self.itos[index] for index in tensor]\n",
    "        return ''.join(tokens)\n",
    "    \n",
    "    def _tokenize(self, s):\n",
    "        return [c for c in s.encode()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"DadJokes/shortjokes.csv\"\n",
    "# DATASET_PATH = \"DadJokes/shortjokes_head.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DadJokesDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path, sep=',')['Joke'].tolist()\n",
    "        self.text_encoder = CharacterEncoder(self.data, append_eos=True)\n",
    "        self.samples = []\n",
    "        for _ in range(len(self.data)):\n",
    "            joke = self.data.pop()\n",
    "            self.samples.append(self.generate_language_model_samples(self.text_encoder.encode(joke)))\n",
    "        del self.data        \n",
    "        \n",
    "    def generate_language_model_samples(self, joke):\n",
    "        res = {}\n",
    "        res['text'] = joke[:-1]\n",
    "        res['next'] = joke[1:]\n",
    "        return res\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]\n",
    "    \n",
    "    def getitem_readable(self, i):\n",
    "        return {'text': self.text_encoder.decode(self.samples[i]['text']),\n",
    "               'next': self.text_encoder.decode(self.samples[i]['next'])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = DadJokesDataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DadJokesGenerator(nn.Module):\n",
    "    def __init__(self, n_chars, char_embedding_dim, lstm_dim, n_lstm_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "        # initialize lookup table of fixed dictionary and size, 0 = padding idx\n",
    "        self.char_embedder = nn.Embedding(self.n_chars, self.char_embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # lstm_dim = size of one lstm hidden layer\n",
    "        # n_lstm_layers = number of stacked lstms\n",
    "        self.lstm = nn.LSTM(char_embedding_dim, lstm_dim, n_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # creates a pipeline to transform the output of the lstm into a list where each position is a char idx\n",
    "        # and its values can be projected by ReLU to be the confidence for each character\n",
    "        self.projection = nn.Sequential(*[\n",
    "            nn.Linear(lstm_dim, self.n_chars),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, char_ids):\n",
    "        # char_ids of shape [b_sz, max_sq_len (w/ pads)]\n",
    "        x = self.char_embedder(char_ids)  # -> shape [b_sz, max_sq_len, char_emb_sz]\n",
    "        x, _ = self.lstm(x)  # -> shape [b_sz, max_sq_len, lstm_dim]  [b_sz, max_sq_len * lstm_dim]\n",
    "        \n",
    "        return self.projection(x)  # -> shape [b_sz, max_sq_len, total_num_characters]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pad_packed_sequence\n",
    "\n",
    "def padded_collate(batch, padding_idx=0):\n",
    "    x = pad_sequence([elem['text'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
    "    y = pad_sequence([elem['next'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
    "    \n",
    "    return {'text': x, 'next': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DadJokesGenerator(dataset.text_encoder.vocab_size, 16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, n_epochs, lr=0.01, batch_size=32, model_checkpoint_folder=None):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='elementwise_mean')\n",
    "    \n",
    "    train_sampler = BucketBatchSampler(dataset, batch_size, True, sort_key=lambda r: len(r['text']))\n",
    "    data_loader = DataLoader(dataset, batch_sampler=train_sampler, collate_fn=padded_collate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    avg_mom=0.98\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        t = tqdm(iter(data_loader), leave=False, total=len(data_loader), miniters=0)\n",
    "        epoch_losses = []\n",
    "        for i, batch in enumerate(t):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            label = batch['next']\n",
    "            pred = model(batch['text'])\n",
    "            \n",
    "            batch_sz, seq_len, n_chars = pred.shape\n",
    "            \n",
    "            loss = criterion(pred.view(batch_sz * seq_len, -1), label.view(batch_sz * seq_len))\n",
    "            \n",
    "            loss_numeric = float(loss)\n",
    "            epoch_losses.append(loss_numeric)\n",
    "            avg_loss = avg_loss * avg_mom + loss_numeric * (1-avg_mom)\n",
    "            debias_loss = avg_loss / (1 - avg_mom**(i+1))\n",
    "            \n",
    "            # ??\n",
    "            lrs = \",\".join([str(param_group['lr']) for param_group in optimizer.param_groups])\n",
    "            # ??\n",
    "            t.set_postfix(loss=debias_loss, \n",
    "                          learning_rate=lrs,\n",
    "                         )\n",
    "\n",
    "            loss.backward()\n",
    "            # update weights using optimizer formula\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = sum(epoch_losses) / len(data_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        plateau_scheduler.step(epoch_loss)\n",
    "        epoch_losses = []\n",
    "        # save model\n",
    "        if model_checkpoint_folder:\n",
    "            torch.save(model, model_checkpoint_folder + \"model_epoch_{}.pt\".format(epoch))\n",
    "    return losses\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_FOLDER = \"/Users/ludovica/Documents/Training_problems/NN/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_CHECKPOINT_FOLDER + \"text_enc.pickle\", 'wb') as fout:\n",
    "    pickle.dump(dataset.text_encoder.stoi, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(MODEL_CHECKPOINT_FOLDER + \"text_enc.pickle\", 'wb') as fout:\n",
    "    pickle.dump(dataset.text_encoder.stoi, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/.pyenv/versions/py3env/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DadJokesGenerator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-03.                     \n",
      "                                                                                   \r"
     ]
    }
   ],
   "source": [
    "losses = train(model, \n",
    "               dataset, \n",
    "               20, \n",
    "               lr=0.01, \n",
    "               batch_size=32, \n",
    "               model_checkpoint_folder=MODEL_CHECKPOINT_FOLDER\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5928\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 10:41 model_epoch_0.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 10:50 model_epoch_1.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:13 model_epoch_10.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:23 model_epoch_11.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:34 model_epoch_12.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:43 model_epoch_13.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:53 model_epoch_14.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 13:04 model_epoch_15.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 13:14 model_epoch_16.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 13:24 model_epoch_17.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 13:34 model_epoch_18.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 13:45 model_epoch_19.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 10:59 model_epoch_2.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:07 model_epoch_3.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:16 model_epoch_4.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:25 model_epoch_5.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:34 model_epoch_6.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:44 model_epoch_7.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 11:54 model_epoch_8.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   134K Nov  8 12:04 model_epoch_9.pt\r\n",
      "-rw-r--r--  1 ludovica  staff   1.0K Nov  8 10:32 text_enc.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {MODEL_CHECKPOINT_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(MODEL_CHECKPOINT_FOLDER + \"model_epoch_19.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DadJokesGenerator(\n",
       "  (char_embedder): Embedding(102, 16, padding_idx=0)\n",
       "  (lstm): LSTM(16, 64, batch_first=True)\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=102, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0cd6cc26e2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_forward(starting_char, model, max_timesteps=1000):\n",
    "    # initialize the hidden state.\n",
    "    letter = torch.LongTensor([dataset.text_encoder.stoi[starting_char]]).unsqueeze(0)\n",
    "    generated = [starting_char]\n",
    "    i = 0\n",
    "    while i < max_timesteps:\n",
    "        # letter of shape [1, sq_len]\n",
    "        x = model.char_embedder(letter)  # -> [1, sq_len, char_embs_dim]\n",
    "        if i == 0:\n",
    "            x, hidden = model.lstm(x)\n",
    "        # Step through the sequence one element at a time.\n",
    "        # after each step, hidden contains the hidden state.\n",
    "        else:\n",
    "            x, hidden = model.lstm(x, hidden)\n",
    "        prediction = F.softmax(model.projection(x), -1)\n",
    "\n",
    "        confidence, letter = prediction.max(-1)\n",
    "\n",
    "        next_letter = dataset.text_encoder.decode(letter)\n",
    "\n",
    "        generated.append(next_letter)\n",
    "        if next_letter == '</s>':\n",
    "            break\n",
    "        i += 1\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add support for start of more than one letter\n",
    "def tell_me_a_joke_starting_with(start):    \n",
    "    return ''.join(generation_forward(start, model, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mat the s\n",
      "Uh...hilarious...\n"
     ]
    }
   ],
   "source": [
    "print(tell_me_a_joke_starting_with('a'))\n",
    "print('Uh...hilarious...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now the generator does not work. Here are the next steps I intend to implement to improve it:\n",
    "1 - Find more data\n",
    "2 - Try dropout and a larger model\n",
    "3 - Try different sampling scheme\n",
    "4 - word encoding\n",
    "5 - pretrain on language modeling on wikipedia data\n",
    "\n",
    "\n",
    "Things that usually seems to be used in generative models but that I do not think make sense to use here:\n",
    "1 - byte encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
