{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dad jokes model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SGLTluK1SFIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download PyTorch\n",
        "\n",
        "PyTorch does not come with CoLab so every time we restart this notebook we have to redownload it."
      ]
    },
    {
      "metadata": {
        "id": "_WMSs6co-65T",
        "colab_type": "code",
        "outputId": "63c0a1f0-c175-4d61-8b45-9b3fdd3104d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.6 (default, Sep 12 2018, 18:26:19) \\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "Ljxs2bw5-681",
        "colab_type": "code",
        "outputId": "13f7f221-ca7f-413e-b1aa-446197e2ba3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxhcgjVI_Lmv",
        "colab_type": "code",
        "outputId": "cd958b99-1d9b-4918-ab13-a18e16a622a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.18.4)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.14.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.22.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.22)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->pytorch-nlp) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3rIPaIBl-7IO",
        "colab_type": "code",
        "outputId": "8f1c381b-5fc3-4dd9-a807-adc0cca315d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wg6EbhumRqrf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wikipedia data for LM pretraining\n",
        "\n",
        "Our dad jokes dataset does not have that many samples so its performance is not great as-is. The major problem with having so few examples is that the model needs to learn both how to make jokes and how to compose words, and this is too much to ask of a small dataset. To learn how words are made, we are downloading a corpus based on Wikipedia to pretrain our model."
      ]
    },
    {
      "metadata": {
        "id": "UDBMzMHFQyv1",
        "colab_type": "code",
        "outputId": "e6d523a1-a1f8-4d37-b0eb-1732f8306ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.marekrei.com/pub/lm-dataset.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-18 06:01:30--  http://www.marekrei.com/pub/lm-dataset.tar.gz\n",
            "Resolving www.marekrei.com (www.marekrei.com)... 217.146.69.7, 2a02:29e8:770:0:3::17\n",
            "Connecting to www.marekrei.com (www.marekrei.com)|217.146.69.7|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76178112 (73M) [application/x-gzip]\n",
            "Saving to: ‘lm-dataset.tar.gz.1’\n",
            "\n",
            "lm-dataset.tar.gz.1 100%[===================>]  72.65M  15.9MB/s    in 4.6s    \n",
            "\n",
            "2018-11-18 06:01:35 (15.9 MB/s) - ‘lm-dataset.tar.gz.1’ saved [76178112/76178112]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMuLamrzRFpm",
        "colab_type": "code",
        "outputId": "a31c7bb5-2718-426a-b385-b1846fb74977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!tar xvzf lm-dataset.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lm-dataset/lm-dataset.train.unk100.txt\n",
            "lm-dataset/lm-dataset.dev.txt\n",
            "lm-dataset/lm-dataset.test.unk100.txt\n",
            "lm-dataset/lm-dataset.test.txt\n",
            "lm-dataset/lm-dataset.train.unk100.top40K.txt\n",
            "lm-dataset/README.md\n",
            "lm-dataset/lm-dataset.dev.unk100.txt\n",
            "lm-dataset/lm-dataset.dev.unk100.top1K.txt\n",
            "lm-dataset/\n",
            "lm-dataset/lm-dataset.train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ejsaz-HURTKY",
        "colab_type": "code",
        "outputId": "43ce36ce-27aa-418e-e7c2-1a4f10bed9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lh lm-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 191M\n",
            "-rw-r--r-- 1 2544 2544  22M Apr  1  2015 lm-dataset.dev.txt\n",
            "-rw-r--r-- 1 2544 2544 117K Apr  4  2015 lm-dataset.dev.unk100.top1K.txt\n",
            "-rw-r--r-- 1 2544 2544  21M Apr  2  2015 lm-dataset.dev.unk100.txt\n",
            "-rw-r--r-- 1 2544 2544  23M Mar 21  2015 lm-dataset.test.txt\n",
            "-rw-r--r-- 1 2544 2544  21M Apr  2  2015 lm-dataset.test.unk100.txt\n",
            "-rw-r--r-- 1 2544 2544  53M Mar 21  2015 lm-dataset.train.txt\n",
            "-rw-r--r-- 1 2544 2544 4.7M Apr 14  2015 lm-dataset.train.unk100.top40K.txt\n",
            "-rw-r--r-- 1 2544 2544  49M Apr  2  2015 lm-dataset.train.unk100.txt\n",
            "-rw-rw-r-- 1 2544 2544  141 Apr  5  2015 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JzmUMWtFSBKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define classes"
      ]
    },
    {
      "metadata": {
        "id": "68Y3QblS-1Cb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import *\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FnioLGzu-1Cf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchnlp.samplers import BucketBatchSampler\n",
        "from torchnlp.datasets import snli_dataset\n",
        "from torchnlp.utils import datasets_iterator\n",
        "from torchnlp.text_encoders import IdentityEncoder, CharacterEncoder, StaticTokenizerEncoder\n",
        "from torchnlp import word_to_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raAsSpRw-1Ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import tqdm as tq\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "\n",
        "\n",
        "def in_ipynb():\n",
        "    try:\n",
        "        cls = get_ipython().__class__.__name__\n",
        "        return cls == 'ZMQInteractiveShell'\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def in_notebook():\n",
        "    try:\n",
        "        from ipykernel.kernelapp import IPKernelApp\n",
        "        return IPKernelApp.initialized()\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def clear_tqdm():\n",
        "    inst = getattr(tq.tqdm, '_instances', None)\n",
        "    if not inst: return\n",
        "    try:\n",
        "        for i in range(len(inst)): inst.pop().close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if in_notebook():\n",
        "    def tqdm(*args, **kwargs):\n",
        "        clear_tqdm()\n",
        "        return tq.tqdm(*args, file=sys.stdout, **kwargs)\n",
        "    def trange(*args, **kwargs):\n",
        "        clear_tqdm()\n",
        "        return tq.trange(*args, file=sys.stdout, **kwargs)\n",
        "else:\n",
        "    from tqdm import tqdm, trange\n",
        "    tnrange=trange\n",
        "    tqdm_notebook=tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gPoLsFNl-1Ck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharacterByteEncoder(StaticTokenizerEncoder):\n",
        "    \"\"\" Encodes text into a tensor by encoding into bytes and splitting the text into individual \n",
        "    characters.\n",
        "\n",
        "    Args:\n",
        "        sample (list of strings): Sample of data to build dictionary on\n",
        "        min_occurrences (int, optional): Minimum number of occurrences for a token to be added to\n",
        "          dictionary.\n",
        "        append_eos (bool, optional): If `True` append EOS token onto the end to the encoded vector.\n",
        "    \"\"\"\n",
        "    # TODO add start and end token\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if 'tokenize' in kwargs:\n",
        "            raise TypeError('CharacterEncoder defines a tokenize callable per character')\n",
        "        super().__init__(*args, tokenize=self._tokenize, **kwargs)\n",
        "\n",
        "    def decode(self, tensor):\n",
        "        tokens = [self.itos[index] for index in tensor]\n",
        "        return ''.join(tokens)\n",
        "    \n",
        "    def _tokenize(self, s):\n",
        "        return [c for c in s.encode()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9BjZ-gnhSbkn",
        "colab_type": "code",
        "outputId": "9f1fae28-f057-4c70-9620-fb8466111335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "PZcggEha-1Cn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"shortjokes.csv\"\n",
        "DATASET_FILE_ID = \"1bplfuUrJEnpi6r78LQtO3IufzCSSVJaC\"\n",
        "DATASET_URL = \"https://github.com/amoudgl/short-jokes-dataset/raw/master/shortjokes.csv\"\n",
        "\n",
        "WIKIPEDIA_DATA_PATH = \"lm-dataset/lm-dataset.train.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5nO_WGraCOPR",
        "colab_type": "code",
        "outputId": "d2c23e26-7be6-4306-8c41-fdbccef6b780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "...\n",
        "# Download the file from `url` and save it locally under `file_name`:\n",
        "urllib.request.urlretrieve(DATASET_URL, DATASET_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('shortjokes.csv', <http.client.HTTPMessage at 0x7fd03161b710>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "NoNo3036-1Cp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DadJokesDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.data = pd.read_csv(path, sep=',')['Joke'].tolist()\n",
        "        self.text_encoder = CharacterEncoder(self.data, append_eos=True)\n",
        "        self.samples = []\n",
        "        for _ in range(len(self.data)):\n",
        "            joke = self.data.pop()\n",
        "            self.samples.append(self.generate_language_model_samples(self.text_encoder.encode(joke)))\n",
        "        del self.data        \n",
        "        \n",
        "    def generate_language_model_samples(self, joke):\n",
        "        res = {}\n",
        "        res['text'] = joke[:-1]\n",
        "        res['next'] = joke[1:]\n",
        "        return res\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.samples[i]\n",
        "    \n",
        "    def getitem_readable(self, i):\n",
        "        return {'text': self.text_encoder.decode(self.samples[i]['text']),\n",
        "               'next': self.text_encoder.decode(self.samples[i]['next'])}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nka0-UYCSryZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        with open(path, 'r') as fin:\n",
        "          self.data = [line.strip() for line in fin.readlines()]\n",
        "        self.text_encoder = CharacterEncoder(self.data, append_eos=True)\n",
        "        self.samples = []\n",
        "        for _ in range(len(self.data)):\n",
        "            joke = self.data.pop()\n",
        "            self.samples.append(self.generate_language_model_samples(self.text_encoder.encode(joke)))\n",
        "        del self.data        \n",
        "        \n",
        "    def generate_language_model_samples(self, joke):\n",
        "        res = {}\n",
        "        res['text'] = joke[:-1]\n",
        "        res['next'] = joke[1:]\n",
        "        return res\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.samples[i]\n",
        "    \n",
        "    def getitem_readable(self, i):\n",
        "        return {'text': self.text_encoder.decode(self.samples[i]['text']),\n",
        "               'next': self.text_encoder.decode(self.samples[i]['next'])}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ign6mp9ZST5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wikipedia_dataset = WikiDataset(WIKIPEDIA_DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBqqFOH9Tvjp",
        "colab_type": "code",
        "outputId": "28cf0556-9db3-4c59-e35e-6cbb179726e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "wikipedia_dataset.getitem_readable(18)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'next': 'e took over from lt gen iqbal khan who proceeded as the vice chief of the army staff ( voas ) , a newly created post .</s>',\n",
              " 'text': 'he took over from lt gen iqbal khan who proceeded as the vice chief of the army staff ( voas ) , a newly created post .'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "pLPtTUxB-1Ct",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DadJokesGenerator(nn.Module):\n",
        "    def __init__(self, n_chars, char_embedding_dim, lstm_dim, n_lstm_layers=1, dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_chars = n_chars\n",
        "        self.char_embedding_dim = char_embedding_dim\n",
        "        \n",
        "        # initialize lookup table of fixed dictionary and size, 0 = padding idx\n",
        "        self.char_embedder = nn.Embedding(self.n_chars, self.char_embedding_dim, padding_idx=0)\n",
        "        \n",
        "        # lstm_dim = size of one lstm hidden layer\n",
        "        # n_lstm_layers = number of stacked lstms\n",
        "        self.lstm = nn.LSTM(char_embedding_dim, lstm_dim, n_lstm_layers, batch_first=True, dropout=dropout)\n",
        "        \n",
        "        # creates a pipeline to transform the output of the lstm into a list where each position is a char idx\n",
        "        # and its values can be projected by ReLU to be the confidence for each character\n",
        "        self.projection = nn.Sequential(*[\n",
        "            nn.Linear(lstm_dim, self.n_chars),\n",
        "            nn.ReLU(),\n",
        "        ])\n",
        "        \n",
        "    def forward(self, char_ids):\n",
        "        # char_ids of shape [b_sz, max_sq_len (w/ pads)]\n",
        "        x = self.char_embedder(char_ids)  # -> shape [b_sz, max_sq_len, char_emb_sz]\n",
        "        x, _ = self.lstm(x)  # -> shape [b_sz, max_sq_len, lstm_dim]  [b_sz, max_sq_len * lstm_dim]\n",
        "        \n",
        "        return self.projection(x)  # -> shape [b_sz, max_sq_len, total_num_characters]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FDx3dS7o-1De",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pad_packed_sequence\n",
        "\n",
        "def padded_collate(batch, padding_idx=0):\n",
        "    x = pad_sequence([elem['text'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "    y = pad_sequence([elem['next'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "    \n",
        "    return {'text': x, 'next': y}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ElN1AaEU-1Dg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = DadJokesGenerator(wikipedia_dataset.text_encoder.vocab_size, 16, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lYKRXbJVDNwl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_LNUf8p-1Di",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, dataset, n_epochs, lr=0.01, batch_size=32, model_checkpoint_folder=None):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='elementwise_mean')\n",
        "    \n",
        "    train_sampler = BucketBatchSampler(dataset, batch_size, True, sort_key=lambda r: len(r['text']))\n",
        "    data_loader = DataLoader(dataset, batch_sampler=train_sampler, collate_fn=padded_collate)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)\n",
        "    \n",
        "    model = model.to(device).train()\n",
        "    \n",
        "    avg_loss = 0.0\n",
        "    avg_mom=0.98\n",
        "    losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        t = tqdm(iter(data_loader), leave=False, total=len(data_loader), miniters=0)\n",
        "        epoch_losses = []\n",
        "        for i, batch in enumerate(t):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            label = batch['next'].to(device)\n",
        "            pred = model(batch['text'].to(device))\n",
        "            \n",
        "            batch_sz, seq_len, n_chars = pred.shape\n",
        "            \n",
        "            loss = criterion(pred.view(batch_sz * seq_len, -1), label.view(batch_sz * seq_len))\n",
        "            \n",
        "            loss_numeric = float(loss)\n",
        "            epoch_losses.append(loss_numeric)\n",
        "            avg_loss = avg_loss * avg_mom + loss_numeric * (1-avg_mom)\n",
        "            debias_loss = avg_loss / (1 - avg_mom**(i+1))\n",
        "            \n",
        "            \n",
        "            lrs = \",\".join([str(param_group['lr']) for param_group in optimizer.param_groups])\n",
        "            \n",
        "            t.set_postfix(loss=debias_loss, \n",
        "                          learning_rate=lrs,\n",
        "                         )\n",
        "\n",
        "            loss.backward()\n",
        "            # update weights using optimizer formula\n",
        "            optimizer.step()\n",
        "        \n",
        "        epoch_loss = sum(epoch_losses) / len(data_loader)\n",
        "        losses.append(epoch_loss)\n",
        "        plateau_scheduler.step(epoch_loss)\n",
        "        epoch_losses = []\n",
        "        # save model\n",
        "        if model_checkpoint_folder:\n",
        "            model_save_path = model_checkpoint_folder + \"model_epoch_{}.pt\".format(epoch)\n",
        "            torch.save(model, model_save_path)\n",
        "    return losses\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y45rAvSw-1Dk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINT_FOLDER = \"checkpoints/\"\n",
        "if not os.path.isdir(MODEL_CHECKPOINT_FOLDER):\n",
        "  os.makedirs(MODEL_CHECKPOINT_FOLDER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxRRPrz7PX9W",
        "colab_type": "code",
        "outputId": "92a90634-d192-46d1-818e-e34e44a5dacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lh checkpoints/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 9.9M\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 01:54 model_epoch_0.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 02:20 model_epoch_1.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 02:46 model_epoch_2.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 03:11 model_epoch_3.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 03:35 model_epoch_4.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 04:00 model_epoch_5.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 04:26 model_epoch_6.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 04:52 model_epoch_7.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 05:16 model_epoch_8.pt\n",
            "-rw-r--r-- 1 root root 1004K Nov 18 05:40 model_epoch_9.pt\n",
            "-rw-r--r-- 1 root root   43K Nov 18 01:27 text_enc.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vOCDdvIu-1Dl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CYomxmU-1Dn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(MODEL_CHECKPOINT_FOLDER + \"text_enc.pickle\", 'wb') as fout:\n",
        "    pickle.dump(wikipedia_dataset.text_encoder.stoi, fout)\n",
        "files.download(MODEL_CHECKPOINT_FOLDER + \"text_enc.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnXGPm4P-1Dt",
        "colab_type": "code",
        "outputId": "754039c5-135a-43c5-c675-9b4ffe65e811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "losses = train(model, \n",
        "               wikipedia_dataset, \n",
        "               30, \n",
        "               lr=0.01, \n",
        "               batch_size=8, \n",
        "               model_checkpoint_folder=MODEL_CHECKPOINT_FOLDER\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DadJokesGenerator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 4339/52409 [02:00<21:24, 37.43it/s, learning_rate=0.01, loss=2.72]Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BQzgxltuaM0j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_save_path = MODEL_CHECKPOINT_FOLDER + \"model_checkpoint_{}.pt\".format('wikipedia')\n",
        "torch.save(model, model_save_path)\n",
        "files.download(model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Twmqktk3-1Dw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -lh {MODEL_CHECKPOINT_FOLDER}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkp3lytG-1Dy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = torch.load(MODEL_CHECKPOINT_FOLDER + \"model_epoch_19.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXkEeMPe-1Dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9i78_NcS-1D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1lTygEJUOgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vgmku86VUOk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3Cy59lWUOn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hP9IgmwJUR5s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Now train on the actual dataset"
      ]
    },
    {
      "metadata": {
        "id": "fyqGsK7vUOjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = DadJokesDataset(DATASET_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPJ9ZIQwUQ3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.text_encoder.stoi['a']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mc4M5Zg-EKP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate"
      ]
    },
    {
      "metadata": {
        "id": "9S5Cyl-wEMZI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initialize the hidden state.\n",
        "starting_char = 'h'\n",
        "\n",
        "def generation_forward(starting_char, model, max_timesteps=1000):\n",
        "    letter = torch.LongTensor([dataset.text_encoder.stoi[starting_char]]).unsqueeze(0)\n",
        "    generated = [starting_char]\n",
        "    i = 0\n",
        "    while i < max_timesteps:\n",
        "        # letter of shape [1, sq_len]\n",
        "        x = model.char_embedder(letter)  # -> [1, sq_len, char_embs_dim]\n",
        "        if i == 0:\n",
        "            x, hidden = model.lstm(x)\n",
        "        # Step through the sequence one element at a time.\n",
        "        # after each step, hidden contains the hidden state.\n",
        "        else:\n",
        "            x, hidden = model.lstm(x, hidden)\n",
        "        prediction = F.softmax(model.projection(x), -1)\n",
        "\n",
        "        confidence, letter = prediction.max(-1)\n",
        "\n",
        "        next_letter = dataset.text_encoder.decode(letter)\n",
        "\n",
        "        generated.append(next_letter)\n",
        "        if next_letter == '</s>':\n",
        "            break\n",
        "        i += 1\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HWcOpbB-1D5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tell_me_a_joke_starting_with(s):\n",
        "    start = []\n",
        "    for c in list(s):\n",
        "        start.append(dataset.text_encoder.stoi[c])\n",
        "    # transform into tensor\n",
        "    encoded_joke = get_joke(start)\n",
        "    joke = dataset.text_encoder.stoi(encoded_joke)\n",
        "    return joke\n",
        "\n",
        "def get_joke(start):\n",
        "    cur = start\n",
        "    while cur[-1] != tensor char of eos:\n",
        "        output = model(cur)\n",
        "        chars_prob = F.softmax(model())\n",
        "        next_char = get_max_char(all_chars)\n",
        "        cur.append(next_char)\n",
        "    return cur"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TkFsc8Q-1D6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.text_encoder.vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hu6LBoau-1D_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample = dataset.text_encoder.encode('a').unsqueeze(0)\n",
        "\n",
        "logits = model(sample)\n",
        "#logits.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vWxq2Dl-1EB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kE5a_wU-1EE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out = F.softmax(logits, -1)\n",
        "out.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2q-RxM--1EG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "confidence, values = out.max(-1)\n",
        "values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYZNMCEP-1EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GUE2yF0d-1EK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FfqPn_1c-1EN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.text_encoder.encode('a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lt15uNWB-1ES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.text_encoder.stoi['a']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9wFWnAGh-1EW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tell_me_a_joke_starting_with('a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvcIlla2-1Ea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "start = np.asarray([10])\n",
        "torch.unsqueeze((torch.FloatTensor([torch.FloatTensor(start)])), 0).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "ujGBUkX4-1Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model((torch.FloatTensor([torch.FloatTensor(start)])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SenFjU6E-1Ef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.text_encoder.itos[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yN0Z3WAv-1Eh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
