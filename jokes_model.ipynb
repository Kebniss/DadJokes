{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jokes_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGLTluK1SFIt",
        "colab_type": "text"
      },
      "source": [
        "# Download PyTorch\n",
        "\n",
        "PyTorch does not come with CoLab so every time we restart this notebook we have to redownload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMSs6co-65T",
        "colab_type": "code",
        "outputId": "0536016f-2887-4a61-940d-f75d101e3a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.6.7 (default, Oct 22 2018, 11:32:17) \\n[GCC 8.2.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljxs2bw5-681",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q torch==1.0.0 torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9Hs3CXBGO8H",
        "colab_type": "code",
        "outputId": "825a5fef-57ed-498b-8f2c-9e07cb7c54ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# we will verify that GPU is enabled for this notebook\n",
        "# following should print: CUDA is available!  Training on GPU ...\n",
        "# \n",
        "# if it prints otherwise, then you need to enable GPU: \n",
        "# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxhcgjVI_Lmv",
        "colab_type": "code",
        "outputId": "a29f0400-a1ed-4bad-9e65-5c95df6baf74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.3.7.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.22.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.18.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->pytorch-nlp) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzmUMWtFSBKg",
        "colab_type": "text"
      },
      "source": [
        "# Define classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Y3QblS-1Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import multiprocessing as mp\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import *\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "import csv\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnioLGzu-1Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchnlp.samplers import BucketBatchSampler\n",
        "from torchnlp.datasets import snli_dataset\n",
        "from torchnlp.utils import datasets_iterator\n",
        "from torchnlp import word_to_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raAsSpRw-1Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import tqdm as tq\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "\n",
        "\n",
        "def in_ipynb():\n",
        "    try:\n",
        "        cls = get_ipython().__class__.__name__\n",
        "        return cls == 'ZMQInteractiveShell'\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def in_notebook():\n",
        "    try:\n",
        "        from ipykernel.kernelapp import IPKernelApp\n",
        "        return IPKernelApp.initialized()\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def clear_tqdm():\n",
        "    inst = getattr(tq.tqdm, '_instances', None)\n",
        "    if not inst: return\n",
        "    try:\n",
        "        for i in range(len(inst)): inst.pop().close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if in_notebook():\n",
        "    def tqdm(*args, **kwargs):\n",
        "        clear_tqdm()\n",
        "        return tq.tqdm(*args, file=sys.stdout, **kwargs)\n",
        "    def trange(*args, **kwargs):\n",
        "        clear_tqdm()\n",
        "        return tq.trange(*args, file=sys.stdout, **kwargs)\n",
        "else:\n",
        "    from tqdm import tqdm, trange\n",
        "    tnrange=trange\n",
        "    tqdm_notebook=tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjHKit2mHvRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Union\n",
        "from collections import deque\n",
        "\n",
        "class CharByteEncoder():\n",
        "  def __init__(self):\n",
        "    self.start_token = '<s>'\n",
        "    self.end_token = '</s>'\n",
        "    self.pad_token = '<pad>'\n",
        "    \n",
        "    self.start_idx = 256\n",
        "    self.end_idx = 257\n",
        "    self.pad_idx = 258\n",
        "    \n",
        "  def encode(self, s: str, pad_to=0) -> torch.LongTensor:\n",
        "    encoded = s.encode()\n",
        "    n_pad = pad_to - len(encoded) if pad_to > len(encoded) else 0\n",
        "    return torch.LongTensor([self.start_idx] + \n",
        "            [c for c in encoded] + \n",
        "            [self.end_idx] + \n",
        "            [self.pad_idx for _ in range(n_pad)]\n",
        "    )\n",
        "  \n",
        "  def decode(self, char_ids_tensor: torch.LongTensor) -> str:\n",
        "    \"\"\"\n",
        "    Decodes a list of char IDs. The structure can be like this:\n",
        "    <s>[body]</s><pad>* so we check for that.\n",
        "    \"\"\"\n",
        "    char_ids = char_ids_tensor.cpu().detach().tolist()\n",
        "    \n",
        "    out = []\n",
        "    buf = []\n",
        "    for c in char_ids:\n",
        "      if c < 256:\n",
        "        buf.append(c)\n",
        "      else:\n",
        "        if buf:\n",
        "          out.append(bytes(buf).decode())\n",
        "          buf = []\n",
        "        if c == self.start_idx:\n",
        "          out.append(self.start_token)\n",
        "        elif c == self.end_idx:\n",
        "          out.append(self.end_token)\n",
        "        elif c == self.pad_idx:\n",
        "          out.append(self.pad_token)\n",
        "      \n",
        "    if buf:  # in case some are left\n",
        "      out.append(bytes(buf).decode())\n",
        "    return ''.join(out)\n",
        "\n",
        "  def __len__(self):\n",
        "    return 259 # 256 combinations in a byte, plus 3 special chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BjZ-gnhSbkn",
        "colab_type": "code",
        "outputId": "ce9e23a3-03f9-46cf-d580-4f5c5fcab7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZcggEha-1Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_PATH = \"shortjokes.csv\"\n",
        "DATASET_FILE_ID = \"1bplfuUrJEnpi6r78LQtO3IufzCSSVJaC\"\n",
        "DATASET_URL = \"https://github.com/amoudgl/short-jokes-dataset/raw/master/shortjokes.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nO_WGraCOPR",
        "colab_type": "code",
        "outputId": "4aa18cc4-317d-4ef1-dfca-3721b4fe6b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import urllib.request\n",
        "...\n",
        "# Download the file from `url` and save it locally under `file_name`:\n",
        "urllib.request.urlretrieve(DATASET_URL, DATASET_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('shortjokes.csv', <http.client.HTTPMessage at 0x7fbbe45b03c8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYJ5vFupRUm",
        "colab_type": "text"
      },
      "source": [
        "PyTorch wants us to define a Dataset class that will be used during training. We are training our language model by having multiple sequences each batch. Each joke needs to stay together for the LSTM to learn the sequential relationship. We are therefore making each sample a joke, with the target being the next character. So if the input is `\"<s>hello!</s>\"` then we are going to have this: `{text: '<s>hello', next: 'hello</s>'}`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoNo3036-1Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DadJokesDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.data = pd.read_csv(path, sep=',')['Joke'].tolist()\n",
        "        self.text_encoder = CharByteEncoder()\n",
        "        self.samples = []\n",
        "        for _ in range(len(self.data)):\n",
        "            joke = self.data.pop()\n",
        "            encoded_joke = self.text_encoder.encode(joke)\n",
        "            encoded_sample = self.generate_language_model_samples(encoded_joke)\n",
        "            self.samples.append(encoded_sample)\n",
        "        del self.data        \n",
        "        \n",
        "    def generate_language_model_samples(self, joke):\n",
        "        \"\"\"\n",
        "        Input: '<s>my funny joke</s>'\n",
        "        Output: {\n",
        "          text: '<s>my funny joke'\n",
        "          next: 'my funny joke</s>'\n",
        "        }\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res['text'] = joke[:-1]\n",
        "        res['next'] = joke[1:]\n",
        "        return res\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.samples[i]\n",
        "    \n",
        "    def getitem_readable(self, i):\n",
        "        return {'text': self.text_encoder.decode(self.samples[i]['text']),\n",
        "               'next': self.text_encoder.decode(self.samples[i]['next'])}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecXV1YvrokZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jokes_dataset = DadJokesDataset(DATASET_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEDyThdsMSOI",
        "colab_type": "text"
      },
      "source": [
        "Checking that the max size of a sequence is not too bad to make sure padding and memory are taken care of"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EyR2x7yvjnZ",
        "colab_type": "code",
        "outputId": "8f9156f6-4352-4acf-e4e3-69f09966b065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max((len(p['text']) for p in jokes_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ApouVmZqQLE",
        "colab_type": "text"
      },
      "source": [
        "Our model is going to be made by multiple LSTM layers stacked on top of each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2TwHB7Rob6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class CharLstmLanguageModel(nn.Module):\n",
        "    def __init__(self, char_embedding_dim, lstm_dim, projection_size, n_lstm_layers=1, dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_lstm_layers = n_lstm_layers\n",
        "        self.n_chars = 259\n",
        "        self.char_embedding_dim = char_embedding_dim\n",
        "        self.projection_size = projection_size\n",
        "        \n",
        "        # initialize lookup table of fixed dictionary and size, 0 = padding idx\n",
        "        self.char_embedder = nn.Embedding(self.n_chars, \n",
        "                                          self.char_embedding_dim, \n",
        "                                          padding_idx=258\n",
        "        )\n",
        "        \n",
        "        # lstm_dim = size of one lstm hidden layer\n",
        "        # n_lstm_layers = number of stacked lstms\n",
        "        \n",
        "        self.lstm = nn.LSTM(char_embedding_dim, \n",
        "                            lstm_dim, \n",
        "                            n_lstm_layers, \n",
        "                            batch_first=True, \n",
        "                            dropout=dropout, \n",
        "                            bidirectional=False,  # We want to use this for generation so we won't have all the seq together\n",
        "        )\n",
        "        \n",
        "        # creates a pipeline to transform the output of the lstm into a list where each position is a char idx\n",
        "        # and its values can be projected by ReLU to be the confidence for each character\n",
        "        self.projection = nn.Sequential(*[\n",
        "            nn.Linear(lstm_dim, self.projection_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.projection_size, self.n_chars),\n",
        "        ])\n",
        "        \n",
        "    def forward(self, char_ids, hidden=None, pack=True):\n",
        "        # char_ids of shape [b_sz, max_sq_len (w/ pads)]\n",
        "        x = self.char_embedder(char_ids)  # -> shape [b_sz, max_sq_len, char_emb_sz]\n",
        "        \n",
        "        x, hidden = self.lstm(x, hidden)  # -> shape [b_sz, max_sq_len, lstm_dim]\n",
        "\n",
        "        return self.projection(x), hidden  # -> projection's shape: [b_sz, max_sq_len, total_num_characters]\n",
        "                                           # -> (seq_len, batch, num_directions * hidden_size)\n",
        "                                           # hidden is a tuple (hidden_state, cell_state). Both are of shape (n_lstm_layers, b_sz, lstm_dim) \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqryQJXpoc_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence\n",
        "\n",
        "def padded_collate(batch, padding_idx=258):\n",
        "    x = pad_sequence([elem['text'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "    y = pad_sequence([elem['next'] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "    \n",
        "    return {'text': x, 'next': y}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqShIRDznDy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharLstmLanguageModel(16,  # char_embedding_dim \n",
        "                              1024,  # lstm_hidden_dim\n",
        "                              512,\n",
        "                              n_lstm_layers=2,\n",
        "                              dropout=0.3,\n",
        "                             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzqATMepnD1m",
        "colab_type": "code",
        "outputId": "72c7a2bc-10fd-44f0-a388-2bb1c64169e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9N3aexRMlTe",
        "colab_type": "text"
      },
      "source": [
        "Let's define a train function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1VqD8vqioP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, dataset, n_epochs, lr=0.01, batch_size=32, model_checkpoint_folder=None):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='elementwise_mean')\n",
        "    \n",
        "    train_sampler = BucketBatchSampler(dataset, batch_size, True, sort_key=lambda r: len(r['text']))\n",
        "    data_loader = DataLoader(dataset, batch_sampler=train_sampler, collate_fn=padded_collate, num_workers=mp.cpu_count())\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)\n",
        "    \n",
        "    model = model.to(device).train()\n",
        "    \n",
        "    avg_loss = 0.0\n",
        "    avg_mom=0.98\n",
        "    losses = []\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        t = tqdm(iter(data_loader), leave=False, total=len(data_loader), miniters=0)\n",
        "        epoch_losses = []\n",
        "        for i, batch in enumerate(t):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            label = batch['next'].to(device)\n",
        "            pred, _ = model(batch['text'].to(device))\n",
        "            \n",
        "            batch_sz, seq_len, n_chars = pred.shape\n",
        "            \n",
        "            loss = criterion(pred.view(batch_sz * seq_len, -1), label.view(batch_sz * seq_len))\n",
        "            \n",
        "            loss_numeric = float(loss)\n",
        "            epoch_losses.append(loss_numeric)\n",
        "            avg_loss = avg_loss * avg_mom + loss_numeric * (1-avg_mom)\n",
        "            debias_loss = avg_loss / (1 - avg_mom**(i+1))\n",
        "            \n",
        "            # ??\n",
        "            lrs = \",\".join([str(param_group['lr']) for param_group in optimizer.param_groups])\n",
        "            # ??\n",
        "            t.set_postfix(loss=debias_loss, \n",
        "                          learning_rate=lrs,\n",
        "                          epoch=epoch,\n",
        "                         )\n",
        "\n",
        "            loss.backward()\n",
        "            # update weights using optimizer formula\n",
        "            optimizer.step()\n",
        "        \n",
        "        epoch_loss = sum(epoch_losses) / len(data_loader)\n",
        "        print(f\"\\nEpoch {epoch} loss: {epoch_loss}\")\n",
        "        losses.append(epoch_loss)\n",
        "        plateau_scheduler.step(epoch_loss)\n",
        "        epoch_losses = []\n",
        "        # save model\n",
        "        if model_checkpoint_folder:\n",
        "            model_save_path = model_checkpoint_folder + \"model_epoch_{}.pt\".format(epoch)\n",
        "            torch.save(model, model_save_path)\n",
        "    return losses\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPyyv6P4qi2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CHECKPOINT_FOLDER = \"checkpoints/\"\n",
        "if not os.path.isdir(MODEL_CHECKPOINT_FOLDER):\n",
        "  os.makedirs(MODEL_CHECKPOINT_FOLDER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-8BiVvLvNnZ",
        "colab_type": "code",
        "outputId": "dcf4be85-4596-49ec-ae48-e0cbb4b38569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharLstmLanguageModel(\n",
              "  (char_embedder): Embedding(259, 16, padding_idx=258)\n",
              "  (lstm): LSTM(16, 1024, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (projection): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.3)\n",
              "    (3): Linear(in_features=512, out_features=259, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9MJbwNZvSQ3",
        "colab_type": "code",
        "outputId": "c1128da7-17ee-4e5d-d422-0e8b02a8dc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar  4 17:49:18 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P8    24W / 149W |     11MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X-Nbgs0qi0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = train(model, \n",
        "               jokes_dataset, \n",
        "               10, \n",
        "               lr=0.01, \n",
        "               batch_size=128, \n",
        "               model_checkpoint_folder=MODEL_CHECKPOINT_FOLDER\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IcbrM0Itu9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "model_save_path = MODEL_CHECKPOINT_FOLDER + \"dad_jokes_model_3epochs_checkpoint_{}.pt\".format('dadjokes_only')\n",
        "torch.save(model, model_save_path)\n",
        "files.download(model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1_PEZ4UyIHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses_djokes = [2.5633907865067753, 2.188173718120328, 2.1247080455767375, 2.091173592755121]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OC_PqLm01R5",
        "colab_type": "code",
        "outputId": "643fac00-7ac9-455f-c23f-2ac0212e099e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses_djokes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbbc6ea05f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokBkXloJlD8",
        "colab_type": "text"
      },
      "source": [
        "# Sampling from the model\n",
        "\n",
        "\n",
        "The model may not have learned how to form valid utf-8 sequences (eg for emojis) so we offer an optional argument that sets all non-ASCII bytes to 0 probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNu4PDguEy89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, start_token='', max_iters=100, temperature=1.0, no_unicode=False):\n",
        "  model = model.to('cpu')\n",
        "  enc = CharByteEncoder()\n",
        "  input_seq = enc.encode(start_token).tolist()[:-1] # Drop the end token\n",
        "  \n",
        "  hidden_prev = None\n",
        " \n",
        "  for t in range(max_iters):\n",
        "    x_t = torch.LongTensor([input_seq[t]]).unsqueeze(0)  # (b_sz=1, seq_len=1)\n",
        "    y_t, hidden_t = model(x_t, hidden_prev)\n",
        "    probs_t = F.softmax(y_t.squeeze() / temperature, dim=0)\n",
        "    \n",
        "    if no_unicode:\n",
        "      mask = torch.cat([torch.ones(128), torch.zeros(128), torch.ones(3)])\n",
        "      probs_t = probs_t * mask\n",
        "\n",
        "    x_next = torch.multinomial(probs_t, 1)  # Randomly picks, based on probabilities\n",
        "\n",
        "    input_seq.append(int(x_next))\n",
        "    if int(x_next) == enc.end_idx:\n",
        "      return enc.decode(torch.LongTensor(input_seq))\n",
        "    hidden_prev = hidden_t\n",
        "    \n",
        "  return enc.decode(torch.LongTensor(input_seq + [enc.end_idx]))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVNhh0ocEy63",
        "colab_type": "code",
        "outputId": "0e32ce9a-373b-44ce-dcbf-af0c4a0edc4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample(model, no_unicode=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>Ktlits I rudiotch nonnine and Recan.</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8a_jCkGthmz",
        "colab_type": "code",
        "outputId": "deeee110-ef82-49d0-d533-d496f770c87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample(model, no_unicode=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>My elarise* the shagt/  y houver...Amwacktla tactunl.</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYRiC52PDZ7",
        "colab_type": "code",
        "outputId": "ea203525-614b-481c-c372-ed254e62cd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample(model, no_unicode=True, start_token='h')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>hIaMcoru vAeH<pad>TNo3e0a </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}